## Ji-won Lee's portfolio

---
### 01. crocodile 데이터를 활용한 악어 학명 예측 모델링
- 프로젝트 소개
  - 본 프로젝트에서는 악어의 형태적, 생태적 데이터를 기반으로 악어의 종을 자동으로 분류할 수 있는 모델을 구축하고자 하였다.
  - 악어류의 데이터를 기반으로 탐색적 분석과 전처리를 수행한 뒤, Random Forest와 XGBoost를 활용해 종(Scientific Name)을 분류하고 주요 영향을 미치는 요인을 도출하고자 하였다. 
- 목표
  - 악어 개체의 길이, 무게, 보전 상태, 서식 지역 등의 특성 데이터를 활용해 종(Scientific Name)을 예측하는 모델을 개발
  - 데이터의 패턴을 시각화하고 분석하여 종 식별에 영향을 주는 요인 도출
- 방법
  - EDA를 통해 결측치 및 이상치 점검, 분포나 상관관계 시각화, 형태적 변수의 로그 변환 및 범주형 변수 인코딩 수행
  - Random Forest 및 XGBoost를 활용하여 모델링하고 Accuracy, Precision, Recall, F1-score를 기준으로 평가
  - Feature importance를 분석하여 종 분류에 영향을 미치는 주요 변수 확인
- 결과 및 의의
  - XGBoost 모델이 약 98%의 정확도로 가장 높은 성능을 보였으며 형태적 특성(Weight, Length)이 종 분류의 핵심 요인임을 확인하였다.
  - 본 프로젝트는 형태적 데이터만으로도 높은 수준의 종 판별이 가능함을 보여주며 생태 연구 및 보전 관리 분야에서의 데이터 기반 의사결정 지원 가능성을 제시한다.
- [자료 다운로드](https://github.com/j11w00n/Portfolio/blob/main/Crocodile_Scientific_Name_Prediction%20(2).ipynb)

### 02. Vehicle 데이터를 활용한 중고차 가격 예측 및 인사이트 분석
- 프로젝트 소개
  - 본 프로젝트에서는 차량의 객관적 특성(연식, 주행거리, 제조사, 사고 이력 등)을 활용하여 중고차 판매 가격을 예측하는 것을 목표로 데이터 기반 회귀 모델을 구축하였다.
  - 약 100만 건의 차량 데이터를 분석하여 데이터 정제, 파생 변수 생성, 모델 학습 및 해석 과정을 거쳤으며 선형 회귀, 랜덤 포레스트, XGBoost 모델을 비교하여 예측 정확성과 설명력을 동시에 확보하고자 하였다.
- 목표
  - 중고 차량의 주요 특성(연식, 주행거리, 제조사, 사고 이력 등)을 활용하여 차량 판매 가격을 예측할 수 있는 회귀 모델 구축
  - 모델의 예측 정확성과 해석 가능성을 모두 고려하여 실제 거래에서 신뢰할 수 있는 가격 산정 근거를 제시
- 방법
  -  약 100만 건의 차량 데이터를 활용하여 결측치 처리, 파생 변수를 생성
  -  EDA를 통한 변수 간 상관관계 분석 및 시각화 및 주요 가격 영향 요인 도출
  -  선형 회귀 -> Random Forest -> XGBoost 순으로 성능(RMSE, R<sup>2</sup>) 비교
  -  XGBoost + SHAP 분석을 통해 변수별 가겨 영향력 시각화 및 해석
- 결과 및 의의
  - XGBoost가 비선형 관계를 효과적으로 반영하며 가장 좋은 성능을 기록하였다. (RMSE=0.06,  R<sup>2</sup>=0.99)
  - SHAP 분석을 통해 제조사, 연식, 주행거리가 가격에 미치는 영향을 시각적으로 확인하며 모델의 해석력 확보을 확보하였다.
  - 데이터 기반으로 차량 가격을 객관적으로 예측함으로써 중고차 거래의 신뢰도 향상 및 자동 가격 선정 시스템의 가능성 제시하였다.
  - 단순 예측을 넘어 SAHP 분석을 통해 '왜 이런 예측이 나왔는가'를 설명할 수 있어 모델 결과에 대한 투명성을 확보하였다.
- [자료 다운로드](https://github.com/j11w00n/Portfolio/blob/main/Vehicle_Price_Prediction%20(1).ipynb)

### 03. MovieLens 데이터를 이용한 영화 추천 시스템 모델링
- 프로젝트 소개
  - 본 프로젝트는 영화 추천 시스템 구축을 목표로 한 모델링 프로젝트이다. 사용자의 평점 데이터를 활용한 SVD 기반 협업 필터링 모델, 영화 줄거리를 활용한 콘텐츠 기반 모델 그리고 하이브리드 추천 모델을 구현하였다.
  - 각 모델의 RMSE 및 MAE를 비교 분석하여 성능을 평가 하였으며 하이브리드 모델은 정량적 지표 대신 추천의 적합성을 중심으로 해석하였다.
- 목표
  - 사용자 개개인의 영화 취향을 반영한 개인화 추천 시스템 구축
  - 다양한 추천 알고리즘을 비교하여 가장 효과적인 추천 방식을 탐색
  - 단순 협업 필터링을 넘어 시간 정보 및 콘텐츠 유사도를 반영한 모델 구축
- 방법
  - MovieLens 데이터셋을 사용하여 필요한 데이터 전처리 후 기본 SVD 모델 구현
  - 이후 하이퍼파라미터를 조정한 Tuned SVD, 저품질 데이터를 제거한 Filtered SVD, 시간 정보를 반영한 Temporal SVD 등 다양한 협업 기반 필터링 추천 시스템 구현
  - 사용자가 분류한 태그 등을 활용한 콘텐츠 기반(TF-IDF) 모델 구현
  - 협업 기반과 콘텐츠 기반을 결합한 하이브리드 모델 구현
  - RMSE, MAE를 중심으로 모델별 예측 정확도 비교
  - 하이브리드 모델은 정량 지표보다 추천 결과의 품질을 해석적으로 평가
- 결과 및 의의
  - Filtered SVD는 기본 SVD 대비 RMSE와 MAE가 개선되어 안정적인 예측 성능을 보였다.
  - TEmporal SVD 역시 RMSE와 MAE가 개선되었으며 이를 통해 사용자가 더 최근에 좋은 점수를 준 영화가 추천 품질에 더 큰 영향을 준다는 결과를 제공하였다.
  - 하이브리드 모델은 협업 필터링의 개인화 특성과 콘텐츠 기반의 유사도 해석력을 결합하여 사용자 취향에 맞는 직관적이고 실용적인 추천 결과를 제공하였다.
  - 본 프로젝트를 통해 추천 시스템의 다양한 접근법을 실제 데이터에 적용하여 비교함으로써 추천 모델의 구조적 차이와 한계점을 명확히 파악할 수 있었다.
  - 특히, 하이브리드 방식은 정량 지표보다 사용자 경험 중신의 평가가 적합함을 확인하였다.
  - 본 프로젝트는 단순한 평점 예측을 넘어 사용자 맞춤형 영화 추천의 방향성과 개선 가능성을 제시하였다.
- [자료 다운로드](https://github.com/j11w00n/Portfolio/blob/main/Movie_Recommendation_System%20(1).ipynb)

### 04. SVHN(Street View House Numbers) 데이터를 이용한 CNN 모델 구현
- 프로젝트 소개
  - 본 프로젝트는 SVHN 데이터셋을 활용한 숫자 이미지 분류 프로젝트이다.
  - Google Street View에서 수집한 집 번호 이미지를 CNN(합성곱신경망)으로 학습하여 10개 클래스의 숫자를 자동으로 인식하는 시스템을 구축하였다.
- 목표
  - 실제 환경에서 촬용된 32x32 픽셀 크기의 숫자 이미지를 10개의 클래스로 정확하게 분류
  - 다양한 조명, 각도, 배경을 가진 실제 이미지에서 높은 정확도 달성
  - 딥러닝 기반 이미지 분류 모델의 실용성 검증
- 방법
  - SVHN 데이터셋을 사용하여 필요한 데이터 전처리 후 3개의 합성곱 블록으로 구성된 CNN 구현
  - 데이터셋을 이용하여 학습 조건 설정 및 모델 학습
  - 학습데이터, 검증데이터, 시험데이터의 평균교차엔트로피및 정확도를 중심으로 모델 평가
- 결과 및 의의
  - Train loss는 지속적이고 안정적인 감소 추세를 보였으며 Validation loss는 초반 급격한 갑소 이후 안정화하는 추세를 보였다.
  - 후반부 과적합이 있지만 심각하지는 않았다.
  - 테스트 샘플(10개)을 이용하여 예측 성능을 평가해 본 결과 10개 중 10개를 모두 정확하게 예측하였다.
  - 특히, 선명한 이미지부터 흐린 이미지까지 다양한 조건에서 안정적인 성능을 보이며 실제 환경의 다양한 변수에 강건한 모델이라는 것을 확인하였다.
  - 본 프로젝트는 향후 주소인식이나 우편 자동 분류 시스템과 같은 자동화 시스템에 적용 가능성을 제시하였다.
- [자료 다운로드](https://github.com/j11w00n/Portfolio/blob/main/CNN_SVHN/CNN_SVHN.pdf)

  ### 05. 네이버 쇼핑 리뷰 데이터를 이용한 LDA 토픽모델링
- 프로젝트 소개
  - 본 프로젝트는 네이버 쇼핑 제품 리뷰 20만 건을 대상으로 LDA 토픽모델링을 적용하여 소비자 리뷰의 주요 주제를 자동으로 분류하고 분석한 프로젝트이다.
  - 한국어 자연어 처리 기술을 활용하여 대량의 비정형 텍스트 데이터에서 의미 있는 패턴을 추출하였다.
- 목표
  - 대규모 제품 리뷰 데이터에서 숨겨진 주제(토픽) 발견
  - 소비자들이 주로 언급하는 관심사와 불만사항 파악
  - 최적의 토픽 개수 선정을 통한 효과적인 리뷰 분류 체계 구축
  - 다양한 평가지표를 통한 토픽모델 성능 검증
- 방법
  - 네이버 쇼핑 리뷰 데이터셋을 사용하여 필요한 데이터 전처리 후 LDA 모델 적용
  - 혼란도, 다양한 응집성지수로 모델의 성능을 다각도로 평가
  - 혼란도와 응집성지수를 기준으로 최적의 토픽 수 탐색 
- 결과 및 의의
  - 초기 모델을 통해 첫번째 토픽에서는 '하나', '느낌', '일단'과 같은 소비자의 주관적인 만족도를 표현하는 패턴을 발견하였다.
  - 초기 모델의 학습데이터와 시험데이터의 혼란도 차이가 적어 과적합 없이 모델이 잘 학습되었음을 확인하였다.
  - 다만 C_v 응집성지수가 0.4 미만으로 나타나 토픽의 의미적 품질 개선이 필요하였다.
  - 최적의 토픽 수를 찾아 토픽 수를 70개로 늘려 모델을 학습한 결과 혼란도 측면에서 훨씬 나은 값을 기록했으며 C_v 응집성지수 역시 0.4 이상으로 준수한 값을 기록하였다.
  - 하지만 UMass, UCI는 오히려 악화되었으며 70개라는 다소 많은 토픽 수로 해석의 용이성을 낮췄다.
  - 이 프로젝트를 통해 소비자들이 제품에 대해 언급하는 다양한 관심사와 불만사항을 체계적으로 파악할 수 있는 기반을 마련하였다.
- [자료 다운로드](https://github.com/j11w00n/Portfolio/blob/main/%ED%86%A0%ED%94%BD%EB%AA%A8%EB%8D%B8%EB%A7%81/%ED%86%A0%ED%94%BD%EB%AA%A8%EB%8D%B8%EB%A7%81_LDA.ipynb)

  ### 06. 20newsgroups 데이터를 이용한 LSA 토픽모델링
- 프로젝트 소개
  - 본 프로젝트는 20newsgroups 데이터셋을 활용하여 LSA 토픽모델링을 수행한 텍스트마이닝 프로젝트이다.
  - 종교, 컴퓨터 그래픽스, 우주과학 관련 4개 카테고리의 뉴스 그룹 게시물을 분석 대상으로 하였다.
- 목표
  - TF-IDF 행렬과 특이값 분해를 활용한 차원 축소 기법 구현
  - 대량의 문서 집합에서 잠재적 의미 구조를 발견하고 주요 토픽 추출
  - 문서와 단어를 저차원 토픽 공간으로 표현하며 의미적 유사성 파악
  - 문서-토픽, 단어-토픽 간의 관계를 정량적으로 분석
- 방법
  - 20newsgroups 데이터셋을 사용하여 필요한 데이터 전처리
  - TF-IDF 행렬을 기반으로 LSA 모델 적용
  - 문서-토픽, 단어-토픽 간의 할당값 확인
- 결과 및 의의
  - 첫번째 토픽의 주요 단어는 god, would, people, think 등으로 종교적 담론과 철학적 사고를 나타내는 추상적 개념 중심이라고 추론할 수 있다.
  - 두번째 토픽의 주요 단어는 file, image, program, format 등으로 컴퓨터 그래픽, 기술 관련 토픽이라고 추론할 수 있다.
  - 세번째 토픽의 주요 단어는 space, file, launch, nasa 등으로 우주과학 관련 토픽이라고 추론할 수 있다.
  - 네번째 토픽의 주요 단어는 beauchaine, bronx, manhattan, bobble 등으로 특정 문서나 이상치에서 나타나는 고유명사에 집중된 토픽이라고 추론할 수 있다.
  - 첫번째 문서에 대하서는 컴퓨터 그래픽, 기술 관련 토픽인 두번째 토픽의 영향력이 가장 강하며 세번째 토픽 역시 상당한 영향력을 보인다.
  - 코퍼스 내에서의 전체적인 중요도는 첫번째 토픽이 가장 강하다.
  - 하지만 4개의 토픽 모두 유의미한 특이값을 보유하고 있다.
  - 이 프로젝트를 통해 대량의 비구조화된 텍스트 데이터에서 주제 자동 추출 시스템 및 의미적으로 유사한 문서 발견 및 추천 시스템 개발 기반을 마련할 수 있다.
- [자료 다운로드](https://github.com/j11w00n/Portfolio/blob/main/%ED%86%A0%ED%94%BD%EB%AA%A8%EB%8D%B8%EB%A7%81/%ED%86%A0%ED%94%BD%EB%AA%A8%EB%8D%B8%EB%A7%81_LSA.ipynb)

  ### 07. MNIST 데이터를 이용한 다층퍼셉트론 모델 구축
- 프로젝트 소개
  - 본 프로젝트는 MNIST 데이터셋을 활용한 손글씨 숫자 이미지를 분류하는 프로젝트이다.
  - 다층퍼셉트론 신경망 모델을 구축하고 과적합 문제를 해결하기 위한 다양한 기법을 비교 분석하였다.
- 목표
  - MNIST 손글씨 숫자를 높은 정확도로 분류하는 딥러닝 모델 구축
  - 과적합 문제를 진단하고 효과적으로 해결
  - 조기 중단, L2 정규화, 드롭아웃 등의 효과 확인 및 비교
  - 학습데이터와 검증/시험데이터 간 성능 차이 최소화
- 방법
  - MNIST 데이터셋을 활용하여 필요한 전처리 수행
  - 5개 층 구조의 다층퍼셉트론 모델 설정 후 100 epoch 학습
  - 기본 모델에 조기 중단 기법 적용
  - 처음 4개의 층에 L2 가중치 규제를 적용한 모델 설정 후 100 epoch 학습
  - 처음 3개의 은닉층 뒤에 20% 드롭아웃을 적용한 모델 설정 후 100 epoch 학습
- 결과 및 의의
  - 기본 모델의 Trian loss가 0에 수렴하며 완벽하게 학습되었지만 Validation loss는 초반 감소 후 다시 증가하는 추세를 보이며 과적합의 패턴을 보였다.
  - 조기중단 기법을 적용하고 모델을 학습하자 epoch 23에서 모델이 중단되어 Test loss와 Validation loss 간의 차이는 줄었지만 여전히 Validation loss가 진동하는 불안정한 패턴을 보임
  - L2 가중치 규제를 적용한 모델은 학습/검증/시험 데이터 간으 성능 차이가 가장 적으며 과적합이 없는 이상적인 학습 패턴을 보였다.
  - 드롭아웃을 적용한 모델은 L2 가중치를 적용한 모델보다 loss 값은 전반적으로 많이 줄었지만 Validation loss가 줄어들다가 다시 늘어나는 추세를 보였다.
  - 이 프로젝트를 통해 과적합 문제에 대한 해결 기법들의 실증적인 비교를 제시해봤으며 L2 정규화 기법이 MNIST 분류 작업에 가장 효과적임을 확인하였다.
- [자료 다운로드](https://github.com/j11w00n/Portfolio/blob/main/%EB%8B%A4%EC%B8%B5%ED%8D%BC%EC%85%89%ED%8A%B8%EB%A1%A0_MNIST.ipynb)
